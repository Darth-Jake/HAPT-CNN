{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activity labels as defined in activity_labels.txt\n",
    "activity_labels = ['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING', 'STAND_TO_SIT', 'SIT_TO_STAND', 'SIT_TO_LIE', 'LIE_TO_SIT', 'STAND_TO_LIE', 'LIE_TO_STAND']\n",
    "activity_labels = {k:v for k,v in enumerate(activity_labels, start=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "def get_data( data_path ):\n",
    "    file = open(data_path)\n",
    "    lines = file.readlines()\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        arr=[]\n",
    "        for x in line.split():\n",
    "            arr.append([float(x)])\n",
    "        data.append(arr)\n",
    "    return data\n",
    "\n",
    "def get_labels( path ):\n",
    "    file = open(path)\n",
    "    lines = file.readlines()\n",
    "    data=[]\n",
    "    for x in lines:\n",
    "        data.append(int(x))\n",
    "    return data\n",
    "    \n",
    "training_data = get_data('HAPT Data Set/Train/X_train.txt')\n",
    "training_labels = get_labels('HAPT Data Set/Train/y_train.txt')\n",
    "test_data = get_data('HAPT Data Set/Test/X_test.txt')\n",
    "test_labels = get_labels('HAPT Data Set/Test/y_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STANDING(5): [[0.04357967369149907], [-0.005970221250665042], [-0.03505434399423213], [-0.9953811604338987], [-0.98836586266332], [-0.9373820052025755], [-0.9950070451399888], [-0.9888155772529817], [-0.9533252009660302], [-0.7947963690882799], [-0.7448928173638687], [-0.6484472451236181], [0.8417955744259182], [0.708440184117894], [0.6517164861985676], [-0.9757520326561472], [-0.9999502178575226], [-0.9998883684449736], [-0.9980136663309187], [-0.9939992936329727], [-0.9919796223936284], [-0.9709700538309324], [-0.5470953808263651], [-0.7009736397656726], [-0.6226971090643219], [0.9218841529145088], [-0.7194828813439718], [0.3421680073898705], [-0.1613176459671459], [0.2660489691636654], [-0.2743514536309294], [0.2672050903154124], [-0.02095842699877548], [0.3826102389765604], [-0.5017480583550334], [0.5124632095302366], [-0.2063368681475705], [0.3767780043836362], [0.4351723171337474], [0.6601985295768886], [0.9600506839216649], [-0.1359387079347199], [0.1155555399323403], [-0.9881344718708118], [-0.9826928441063848], [-0.9197226753589018], [-0.9883619121972991], [-0.9855229581524183], [-0.9318337788964008], [0.8920545077155717], [-0.1613504487732803], [0.1246597706928379], [0.9774363133866788], [-0.1215128105923979], [0.05611428277184594], [-0.3744642205537022], [0.8918214846565797], [-0.9708355506452164], [-0.9755107381799317], [-0.9891408077893051], [-0.9904443114901348], [-0.9488869567947107], [-1.0], [-1.0], [0.03424855949822026], [-0.5931462121537188], [0.5946358701503056], [-0.5964398994243458], [0.5985603074498582], [-0.7122091024145314], [0.7096303968475064], [-0.7074844838254357], [0.7050004977599369], [-0.9921170528612682], [0.9926468658362884], [-0.992885425728154], [0.9917478528343511], [0.5702687557023867], [0.439065091362705], [0.9869129606540701], [0.07799634457269122], [0.005000803103098583], [-0.0977895909043901], [-0.9935190598036159], [-0.9883599877093299], [-0.9935749692407557], [-0.9944373807081907], [-0.9862066390473163], [-0.9928183539140284], [-0.9851800953374004], [-0.9919942317702399], [-0.9931188651129554], [0.9898347102677045], [0.9919568628444668], [0.9905192036599244], [-0.9936581638042379], [-0.9999348675782745], [-0.9998204462024932], [-0.99987846483064], [-0.994406778818829], [-0.9860248681581887], [-0.9886976352605871], [-0.8199492514530194], [-0.793046449350747], [-0.8631452188603261], [0.9480263349326286], [-0.3348070520453087], [0.7142019926966372], [0.4986860901087742], [0.296680795416254], [-0.1895127199290552], [0.3109116344239076], [0.2371100925580052], [0.4109366212511427], [-0.5094751107097448], [0.4755736423467778], [-0.3522073657137417], [-0.3946308852388821], [-0.0167904698040573], [-0.1582366798021562], [-0.006100848902652611], [0.007514728242995705], [-0.06386164447849352], [-0.9853121053064027], [-0.9766075686504224], [-0.9933188302183532], [-0.9845891483662795], [-0.9763264792048038], [-0.9933414266163094], [-0.8975630700857272], [-0.9366759837746599], [-0.7626443422219144], [0.8542971671543522], [0.8950315927771157], [0.8308405402888694], [-0.9671216223465435], [-0.9995782107263746], [-0.9993543267751077], [-0.9998180971825849], [-0.9843995316201017], [-0.9785768028178556], [-0.993052695276511], [0.08263168199427362], [0.1745154353624079], [-0.2184369954592925], [0.3209261253112825], [-0.4526035672745875], [0.46613254170275], [-0.2036323099788325], [1.0], [-1.0], [0.3522759638072726], [0.4483984093924061], [0.757793126851549], [-0.7824647809516742], [0.7037246898512834], [-0.399106532291227], [0.7122741403117447], [0.1356038777641289], [0.3027773473525], [-0.09916739989351753], [-0.05551736887315728], [-0.06198579653958702], [-0.9919182617796988], [-0.992519272732904], [-0.9923523596322019], [-0.9919614692045063], [-0.994949184953057], [-0.9928349510421147], [-0.9917657441326266], [-0.9865647738018454], [-0.9918936329895909], [0.994225212990494], [0.9917610403693489], [0.9893519468213847], [-0.9945225050102361], [-0.9999368088721802], [-0.9999535029287209], [-0.9999246937139927], [-0.9922144049070786], [-0.9969380489934397], [-0.9925839093428056], [-0.5898509631168173], [-0.7098364920849747], [-0.5721068605073568], [0.3746613310946296], [-0.306094433907695], [0.33550550812192], [-0.07930717211408245], [0.9895595990276158], [-0.4549700926180479], [0.3844351547742977], [0.3076382225479752], [0.936231821325072], [-0.6662816555077822], [0.7722016779751506], [0.529584894665172], [-0.1472212065952542], [-0.0944620118561349], [0.351206261651638], [-0.9695463366438416], [-0.9615397390242743], [-0.9676458387434783], [-0.947236200139565], [-0.9941744025358147], [-0.9695463366438416], [-0.9991500016883648], [-0.9705253957854637], [-0.2196195595422423], [-0.1240720476736897], [-0.05824084467969215], [0.1743063527109612], [0.2016204640967148], [-0.9695463366438416], [-0.9615397390242743], [-0.9676458387434783], [-0.947236200139565], [-0.9941744025358147], [-0.9695463366438416], [-0.9991500016883648], [-0.9705253957854637], [-0.2196195595422423], [-0.1240720476736897], [-0.05824084467969215], [0.1743063527109612], [0.2016204640967148], [-0.9940882805733021], [-0.994309238591255], [-0.9944213917763292], [-0.9924355582921777], [-0.9912084654069793], [-0.9940882805733021], [-0.9999010073592248], [-0.9929718631939888], [-0.863283734660203], [0.3032769402821689], [-0.2661685803351152], [-0.1997241912225112], [0.0675597922032074], [-0.968904091465387], [-0.9643351789924111], [-0.9596376408209124], [-0.9751082383600711], [-0.9950089798518805], [-0.968904091465387], [-0.999286270369718], [-0.9548639239425041], [0.1894200691188597], [0.5568269727140553], [-0.8303686133852524], [0.3021982444348863], [0.435743292404956], [-0.994299893790643], [-0.9913676110594047], [-0.9931429759513183], [-0.9889356284332861], [-0.9934860310634928], [-0.994299893790643], [-0.9999491737524392], [-0.9945471780790457], [-0.6197676327726109], [0.3291235646753941], [-0.1496817416803898], [-0.2183940681232841], [-0.03758838665741482], [-0.9948098337697846], [-0.9829840993432242], [-0.9394553430948932], [-0.9952720040507134], [-0.989848258245692], [-0.9403344905219694], [-0.9968243542204652], [-0.984703069207913], [-0.9334087845317396], [-0.9936637369375264], [-0.9902174009528543], [-0.942331996372906], [-0.9939526643813138], [-0.9934478913698704], [-0.923441387476666], [-0.9747327056620144], [-0.9999688338689382], [-0.9998504082294145], [-0.9973083918825506], [-0.9967675855275625], [-0.9897088891746108], [-0.9908072943470555], [-0.9463569204627208], [-0.9047477630320366], [-0.5913024753416221], [-1.0], [-1.0], [-1.0], [0.3620816443137016], [0.1591911409542155], [0.04834979009174112], [0.1417700789113365], [-0.15140242862659], [-0.2606485230958903], [-0.5515418970245799], [0.2532136423120006], [-0.01960428675284032], [-0.9999625745937579], [-0.9999873552144461], [-0.9999790683145872], [-0.9999624505331199], [-0.9999322238126448], [-0.999726224308194], [-0.9996703327969981], [-0.9999858248394896], [-0.9999682393028979], [-0.9999768595408525], [-0.9998685360341643], [-0.9997760928993233], [-0.999971375089116], [-0.999917656416966], [-0.9998850114227521], [-0.9998604643327444], [-0.99986893528082], [-0.9998621335258165], [-0.9997458940808842], [-0.9997213461768957], [-0.9994930995610967], [-0.999813633829762], [-0.9998608364378361], [-0.9998382636257117], [-0.9997418638122828], [-0.999615143461305], [-0.9998557521473173], [-0.9998386292367988], [-0.9974223543137064], [-0.9994705778940255], [-0.9996620429790506], [-0.9996422982151132], [-0.9992968916023078], [-0.9978922191386453], [-0.9959322573167285], [-0.9955139677619667], [-0.997366749903151], [-0.9996850818095266], [-0.9989177936864576], [-0.9956671468712063], [-0.9973479054100713], [-0.9994538175205322], [-0.992332448203644], [-0.9871699053091315], [-0.9896436914568109], [-0.9958206789097965], [-0.9909363057962167], [-0.9964623116428496], [-0.9938054712322565], [-0.9905186916004917], [-0.9971242215452857], [-0.9967401583178773], [-0.991975155383214], [-0.9931567271154407], [-0.9983532983361227], [-0.9911077138455753], [-0.9598964026215066], [-0.9909844165035114], [-0.9999347533353345], [-0.9998204803281749], [-0.9998844920499803], [-0.9921641781630672], [-0.991419597844497], [-0.9955023439263688], [-1.0], [-1.0], [-1.0], [1.0], [-0.24], [-1.0], [0.7007927165391876], [0.2123898779108384], [0.3644364404351352], [-0.717928666107327], [-0.920370674925627], [-0.6438436957503046], [-0.9363100540097138], [-0.4838026856762646], [-0.8186798013842523], [-0.9999865765290177], [-0.9999803941931258], [-0.9999747836111772], [-0.9999549855307795], [-0.9999186054600926], [-0.9996400393924726], [-0.9994840907318486], [-0.9999608659723113], [-0.9999825486126904], [-0.9999712940238522], [-0.9998168159296791], [-0.9994872493667986], [-0.9999792147904241], [-0.9998531261077037], [-0.9999326097066834], [-0.9998999273831223], [-0.999824441893378], [-0.9998571170985388], [-0.9997328877804775], [-0.9997271825941922], [-0.9995670732330683], [-0.9997652447996687], [-0.9999002539395732], [-0.9998128267271502], [-0.9997052365671742], [-0.9995960806361621], [-0.9998521614595248], [-0.9998221009431467], [-0.9993988848231414], [-0.9997634017653562], [-0.9999584581971608], [-0.9999438429696748], [-0.9998246595110155], [-0.9998135089108895], [-0.9987758586653794], [-0.9985777753691059], [-0.9996174107398615], [-0.9999839041750527], [-0.9998324215879144], [-0.998676210236596], [-0.9998443378398282], [-0.9999223001516989], [-0.9865897038988959], [-0.9817710127713637], [-0.9893558566978642], [-0.9850276742384323], [-0.9739069900726591], [-0.994979153939118], [-0.9873396635623599], [-0.9836521593139382], [-0.9923086801792851], [-0.9819478630856762], [-0.9722978537615065], [-0.9953797296895592], [-0.9975688772876207], [-0.9840870881481353], [-0.9943263683706421], [-0.9852593037383314], [-0.999863715250939], [-0.9996660352065035], [-0.9999531280122572], [-0.9902168445514322], [-0.9948272016995018], [-0.9944115802838144], [-0.7124022515574684], [-0.6448423558412073], [-0.8389929779074174], [-1.0], [-1.0], [-1.0], [-0.2169076359462159], [0.09316736088011157], [0.4586560416352614], [0.373538460703625], [0.1301123813985883], [0.2733305262751273], [-0.08691793879896792], [-0.4964863159819886], [-0.7791793475355713], [-0.9998650304966198], [-0.9999322133153893], [-0.9999721859638633], [-0.9999704812760543], [-0.9999317598627695], [-0.9999585366009397], [-0.9999294201330524], [-0.9999846509755966], [-0.999863443187003], [-0.9999668875175348], [-0.9999369742479239], [-0.9999539633792923], [-0.9998644197201493], [-0.9999618917080821], [-0.9995685111872328], [-0.9999780680667092], [-0.999991587447886], [-0.9999902070914126], [-0.9999686104053165], [-0.9998072851977805], [-0.9983461179997852], [-0.9989612201736067], [-0.999618659946474], [-0.9999893442239358], [-0.9999358142841094], [-0.9983880089051727], [-0.9996426840699782], [-0.9999726800719166], [-0.999969822541328], [-0.9999755758502338], [-0.9999057784593973], [-0.9999861086240907], [-0.9999371179129622], [-0.9997511522401715], [-0.9990722735005465], [-0.9999275356399272], [-0.9999657402860814], [-0.9999092028137624], [-0.9998929234830114], [-0.9994443301680073], [-0.9999576305397576], [-0.9999587496897426], [-0.951549871049651], [-0.9665375688167929], [-0.9485238940356663], [-0.9815748459824524], [-0.921602883970196], [-0.951549871049651], [-0.9989894776583105], [-0.9720065614530055], [-0.6463764455299703], [-0.8421052631578947], [-0.06162903424831034], [-0.4461202332343028], [-0.7970460063876168], [-0.9936102720895502], [-0.9942260581609755], [-0.9928386507104064], [-0.9937780519749682], [-0.988171896076191], [-0.9936102720895502], [-0.9999181159849735], [-0.9917359049187399], [-1.0], [-0.9365079365079365], [0.3492603438607211], [-0.5171268446789448], [-0.8010058323401155], [-0.9801348520176819], [-0.961300846286415], [-0.9741289894316136], [-0.9560128787445233], [-0.9898937516542888], [-0.9801348520176819], [-0.999240351714366], [-0.992673389279704], [-0.7012914081357621], [-1.0], [-0.1324799968965547], [0.5656970894074791], [0.3634775179301506], [-0.9919940641616664], [-0.9908771653672619], [-0.9901691951604279], [-0.992520986928416], [-0.9910440021345528], [-0.9919940641616664], [-0.9999367562155236], [-0.9905369250925784], [-0.871305804810962], [-1.0], [-0.01223589440076589], [-0.3148483525480508], [-0.7133078118771379], [-0.1127543407810928], [0.0304003718942103], [-0.4647613856529146], [-0.01844588426675986], [-0.8415585106028753], [0.1799128108076518], [-0.05171841605239957]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test print\n",
    "for i in range(1):\n",
    "    print(f'{activity_labels[training_labels[i]]}({training_labels[i]}): {training_data[i]}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 559, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 279, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 279, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8928)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                571456    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                780       \n",
      "=================================================================\n",
      "Total params: 572,364\n",
      "Trainable params: 572,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(32, 3, strides=1, activation='relu', input_shape=(561,1)))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(len(activity_labels)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(optimizer='adam',\n",
    "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_acc = model.evaluate(test_data, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_outs = model.predict(test_data)\n",
    "print(pred_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(training_data, \n",
    "          training_labels, \n",
    "          epochs=10, \n",
    "          validation_data=(test_data, test_labels))\n",
    "\n",
    "test_loss,test_acc = model.evaluate( test_data, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
